## todo:
[18] why? natural
language supervision in CLIP-ViTâ€™s pre-training, typically
more robust to overfitting by optimizing the similarity between image features and text prompt embeddings
